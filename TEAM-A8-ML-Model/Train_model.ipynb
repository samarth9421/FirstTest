{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
      "display_name": "Python 3.8.5 64-bit"
    },
    "colab": {
      "name": "Train_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwoccCDzd8p6",
        "outputId": "c3376e8a-bd24-4981-ec70-9ce0ca787443"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxb3BQGnd8Rz",
        "outputId": "a67ec46c-6962-4c52-eacf-8a3e0e6db2e4"
      },
      "source": [
        "%cd /content/drive/MyDrive/Intern_data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Intern_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-7cNWLPeECV",
        "outputId": "1bd4e48a-6cdb-41b9-d438-41b10de6e3ab"
      },
      "source": [
        "!pip install aletheia-dnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aletheia-dnn in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from aletheia-dnn) (0.11.1)\n",
            "Requirement already satisfied: csaps>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from aletheia-dnn) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from aletheia-dnn) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from aletheia-dnn) (0.29.23)\n",
            "Requirement already satisfied: pandas>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aletheia-dnn) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from aletheia-dnn) (0.24.2)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from aletheia-dnn) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.9.0->aletheia-dnn) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.2->aletheia-dnn) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.2->aletheia-dnn) (2018.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->aletheia-dnn) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->aletheia-dnn) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->aletheia-dnn) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->aletheia-dnn) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->aletheia-dnn) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.2->aletheia-dnn) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install mpld3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3HWAxyZeGlo",
        "outputId": "5af57566-4d92-4c17-89a9-f74c18fbd820"
      },
      "source": [
        "!pip install pickle5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ4JToqid7Yi"
      },
      "source": [
        "import joblib\n",
        "import pickle5 as pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import make_scorer, roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
        "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from aletheia import *\n",
        "from lime import lime_tabular\n",
        "import mpld3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZqrmkMHf2pq"
      },
      "source": [
        "# Reading Data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAZ4X0WZd7Ym"
      },
      "source": [
        "#read dataset\n",
        "with open('x_res.pkl', 'rb') as input:\n",
        "    x_res = pickle.load(input)\n",
        "    \n",
        "with open('y_res.pkl', 'rb') as input:\n",
        "    y_res = pickle.load(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPPNvuT4d7Yn",
        "outputId": "37eb5c74-6f9d-4ebd-a717-a3f789d724d6"
      },
      "source": [
        "type(x_res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0u1rToxd7Yn",
        "outputId": "51afd80b-1481-4c2a-ee07-f7e23660485c"
      },
      "source": [
        "x_res.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49650, 26)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYgXK9AHd7Yn",
        "outputId": "d73d729d-feb3-4ca1-8b76-dcd3972550a2"
      },
      "source": [
        "fnames = x_res.columns.tolist()\n",
        "print(fnames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'NAME_EDUCATION_TYPE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'LIVE_CITY_NOT_WORK_CITY_1', 'FLAG_OWN_CAR_Y', 'FLAG_DOCUMENT_8_1', 'FLAG_WORK_PHONE_1', 'REGION_RATING_CLIENT_2', 'REGION_RATING_CLIENT_3', 'NAME_CONTRACT_TYPE_Revolving loans', 'FLAG_PHONE_1', 'REG_CITY_NOT_WORK_CITY_1', 'LIVE_REGION_NOT_WORK_REGION_1', 'FLAG_EMAIL_1', 'FLAG_EMP_PHONE_1', 'FLAG_OWN_REALTY_Y', 'REG_CITY_NOT_LIVE_CITY_1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1EEx6ECd7Yo",
        "outputId": "7ac7672d-b04d-4e7d-fdc4-0aeaf0f963d8"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(x_res, y_res, test_size=0.4,random_state=42)\n",
        "\n",
        "#Taking some test samples out for later\n",
        "X_test = X_val[10000:]\n",
        "y_test = y_val[10000:]\n",
        "X_val = X_val[:10000]\n",
        "y_val = y_val[:10000]\n",
        "\n",
        "print(\"X_train shape: \",X_train.shape)\n",
        "print(\"y_train shape: \",y_train.shape)\n",
        "print(\"X_val shape: \",X_val.shape)\n",
        "print(\"y_val shape: \",y_val.shape)\n",
        "print(\"X_test shape: \",X_test.shape)\n",
        "print(\"X_test shape: \",X_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape:  (29790, 26)\n",
            "y_train shape:  (29790, 1)\n",
            "X_val shape:  (10000, 26)\n",
            "y_val shape:  (10000, 1)\n",
            "X_test shape:  (9860, 26)\n",
            "X_test shape:  (9860, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXL--ysWd7Yo"
      },
      "source": [
        "#Save test samples\n",
        "with open('X_test.pkl', 'wb') as output:\n",
        "    joblib.dump(X_test, output)\n",
        "with open('y_test.pkl', 'wb') as output:\n",
        "    joblib.dump(y_test, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Bu6BgXgCQI"
      },
      "source": [
        "# Training Inital DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xydrv2FUd7Yp",
        "outputId": "a95173e0-3a8a-4e8c-f933-5274538fac41"
      },
      "source": [
        "#initial unoptimized DNN \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "metrics = [\n",
        "           keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "           keras.metrics.AUC(name='auc')\n",
        "          ]\n",
        "\n",
        "ES = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\",patience=20,restore_best_weights=True, mode='max', verbose=1)\n",
        "Rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,\n",
        "                              patience=10, min_lr=0.000001)\n",
        "callbacks = [ES, Rlr]\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(32,activation='relu',input_shape=(X_train.shape[1],)))\n",
        "model.add(keras.layers.Dense(16,activation='relu'))\n",
        "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = metrics)\n",
        "model.fit(\n",
        "          x=X_train,\n",
        "          y=y_train,\n",
        "          epochs = 500,\n",
        "          batch_size=512,\n",
        "          validation_data =(X_val, y_val),\n",
        "          callbacks=[ES, Rlr],\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "59/59 [==============================] - 1s 8ms/step - loss: 0.6658 - accuracy: 0.6027 - auc: 0.6402 - val_loss: 0.6260 - val_accuracy: 0.6584 - val_auc: 0.7145\n",
            "Epoch 2/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6131 - accuracy: 0.6677 - auc: 0.7259 - val_loss: 0.5897 - val_accuracy: 0.6837 - val_auc: 0.7495\n",
            "Epoch 3/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5889 - accuracy: 0.6842 - auc: 0.7451 - val_loss: 0.5711 - val_accuracy: 0.7002 - val_auc: 0.7587\n",
            "Epoch 4/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5735 - accuracy: 0.6939 - auc: 0.7531 - val_loss: 0.5604 - val_accuracy: 0.7008 - val_auc: 0.7618\n",
            "Epoch 5/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5632 - accuracy: 0.6996 - auc: 0.7579 - val_loss: 0.5528 - val_accuracy: 0.7092 - val_auc: 0.7644\n",
            "Epoch 6/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5560 - accuracy: 0.7031 - auc: 0.7626 - val_loss: 0.5479 - val_accuracy: 0.7083 - val_auc: 0.7671\n",
            "Epoch 7/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5511 - accuracy: 0.7064 - auc: 0.7664 - val_loss: 0.5450 - val_accuracy: 0.7110 - val_auc: 0.7689\n",
            "Epoch 8/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5473 - accuracy: 0.7082 - auc: 0.7693 - val_loss: 0.5434 - val_accuracy: 0.7141 - val_auc: 0.7700\n",
            "Epoch 9/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5446 - accuracy: 0.7103 - auc: 0.7716 - val_loss: 0.5408 - val_accuracy: 0.7132 - val_auc: 0.7724\n",
            "Epoch 10/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5418 - accuracy: 0.7117 - auc: 0.7743 - val_loss: 0.5393 - val_accuracy: 0.7143 - val_auc: 0.7734\n",
            "Epoch 11/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5396 - accuracy: 0.7120 - auc: 0.7765 - val_loss: 0.5388 - val_accuracy: 0.7172 - val_auc: 0.7741\n",
            "Epoch 12/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5375 - accuracy: 0.7150 - auc: 0.7787 - val_loss: 0.5370 - val_accuracy: 0.7175 - val_auc: 0.7755\n",
            "Epoch 13/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5366 - accuracy: 0.7158 - auc: 0.7794 - val_loss: 0.5365 - val_accuracy: 0.7151 - val_auc: 0.7762\n",
            "Epoch 14/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.7149 - auc: 0.7802 - val_loss: 0.5362 - val_accuracy: 0.7186 - val_auc: 0.7760\n",
            "Epoch 15/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7165 - auc: 0.7809 - val_loss: 0.5356 - val_accuracy: 0.7193 - val_auc: 0.7765\n",
            "Epoch 16/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5342 - accuracy: 0.7176 - auc: 0.7815 - val_loss: 0.5350 - val_accuracy: 0.7176 - val_auc: 0.7769\n",
            "Epoch 17/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7171 - auc: 0.7820 - val_loss: 0.5350 - val_accuracy: 0.7177 - val_auc: 0.7768\n",
            "Epoch 18/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7182 - auc: 0.7827 - val_loss: 0.5345 - val_accuracy: 0.7187 - val_auc: 0.7772\n",
            "Epoch 19/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7187 - auc: 0.7836 - val_loss: 0.5345 - val_accuracy: 0.7184 - val_auc: 0.7771\n",
            "Epoch 20/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5316 - accuracy: 0.7196 - auc: 0.7841 - val_loss: 0.5338 - val_accuracy: 0.7182 - val_auc: 0.7775\n",
            "Epoch 21/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5309 - accuracy: 0.7193 - auc: 0.7845 - val_loss: 0.5337 - val_accuracy: 0.7184 - val_auc: 0.7775\n",
            "Epoch 22/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5300 - accuracy: 0.7205 - auc: 0.7856 - val_loss: 0.5336 - val_accuracy: 0.7186 - val_auc: 0.7774\n",
            "Epoch 23/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5297 - accuracy: 0.7208 - auc: 0.7859 - val_loss: 0.5334 - val_accuracy: 0.7191 - val_auc: 0.7776\n",
            "Epoch 24/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5294 - accuracy: 0.7204 - auc: 0.7862 - val_loss: 0.5332 - val_accuracy: 0.7189 - val_auc: 0.7777\n",
            "Epoch 25/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5291 - accuracy: 0.7205 - auc: 0.7865 - val_loss: 0.5332 - val_accuracy: 0.7183 - val_auc: 0.7776\n",
            "Epoch 26/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5288 - accuracy: 0.7209 - auc: 0.7867 - val_loss: 0.5330 - val_accuracy: 0.7187 - val_auc: 0.7777\n",
            "Epoch 27/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5286 - accuracy: 0.7213 - auc: 0.7870 - val_loss: 0.5328 - val_accuracy: 0.7196 - val_auc: 0.7778\n",
            "Epoch 28/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5282 - accuracy: 0.7212 - auc: 0.7872 - val_loss: 0.5326 - val_accuracy: 0.7194 - val_auc: 0.7782\n",
            "Epoch 29/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5281 - accuracy: 0.7210 - auc: 0.7873 - val_loss: 0.5325 - val_accuracy: 0.7202 - val_auc: 0.7781\n",
            "Epoch 30/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5277 - accuracy: 0.7218 - auc: 0.7877 - val_loss: 0.5324 - val_accuracy: 0.7210 - val_auc: 0.7780\n",
            "Epoch 31/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5274 - accuracy: 0.7216 - auc: 0.7878 - val_loss: 0.5325 - val_accuracy: 0.7186 - val_auc: 0.7780\n",
            "Epoch 32/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5271 - accuracy: 0.7217 - auc: 0.7883 - val_loss: 0.5324 - val_accuracy: 0.7202 - val_auc: 0.7779\n",
            "Epoch 33/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5269 - accuracy: 0.7217 - auc: 0.7884 - val_loss: 0.5323 - val_accuracy: 0.7204 - val_auc: 0.7780\n",
            "Epoch 34/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5268 - accuracy: 0.7220 - auc: 0.7886 - val_loss: 0.5322 - val_accuracy: 0.7202 - val_auc: 0.7780\n",
            "Epoch 35/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5267 - accuracy: 0.7221 - auc: 0.7886 - val_loss: 0.5322 - val_accuracy: 0.7196 - val_auc: 0.7781\n",
            "Epoch 36/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5265 - accuracy: 0.7212 - auc: 0.7887 - val_loss: 0.5321 - val_accuracy: 0.7203 - val_auc: 0.7782\n",
            "Epoch 37/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5264 - accuracy: 0.7215 - auc: 0.7888 - val_loss: 0.5320 - val_accuracy: 0.7210 - val_auc: 0.7783\n",
            "Epoch 38/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5263 - accuracy: 0.7218 - auc: 0.7889 - val_loss: 0.5320 - val_accuracy: 0.7201 - val_auc: 0.7782\n",
            "Epoch 39/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5261 - accuracy: 0.7220 - auc: 0.7890 - val_loss: 0.5320 - val_accuracy: 0.7206 - val_auc: 0.7782\n",
            "Epoch 40/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5260 - accuracy: 0.7219 - auc: 0.7892 - val_loss: 0.5319 - val_accuracy: 0.7209 - val_auc: 0.7783\n",
            "Epoch 41/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5259 - accuracy: 0.7219 - auc: 0.7893 - val_loss: 0.5317 - val_accuracy: 0.7213 - val_auc: 0.7784\n",
            "Epoch 42/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.7222 - auc: 0.7894 - val_loss: 0.5317 - val_accuracy: 0.7211 - val_auc: 0.7784\n",
            "Epoch 43/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5256 - accuracy: 0.7223 - auc: 0.7895 - val_loss: 0.5317 - val_accuracy: 0.7209 - val_auc: 0.7783\n",
            "Epoch 44/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5256 - accuracy: 0.7218 - auc: 0.7896 - val_loss: 0.5317 - val_accuracy: 0.7207 - val_auc: 0.7784\n",
            "Epoch 45/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5255 - accuracy: 0.7216 - auc: 0.7896 - val_loss: 0.5316 - val_accuracy: 0.7207 - val_auc: 0.7784\n",
            "Epoch 46/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5255 - accuracy: 0.7221 - auc: 0.7897 - val_loss: 0.5317 - val_accuracy: 0.7201 - val_auc: 0.7782\n",
            "Epoch 47/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5254 - accuracy: 0.7219 - auc: 0.7897 - val_loss: 0.5317 - val_accuracy: 0.7207 - val_auc: 0.7783\n",
            "Epoch 48/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5253 - accuracy: 0.7214 - auc: 0.7898 - val_loss: 0.5317 - val_accuracy: 0.7207 - val_auc: 0.7782\n",
            "Epoch 49/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5253 - accuracy: 0.7217 - auc: 0.7898 - val_loss: 0.5316 - val_accuracy: 0.7208 - val_auc: 0.7783\n",
            "Epoch 50/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5252 - accuracy: 0.7219 - auc: 0.7898 - val_loss: 0.5316 - val_accuracy: 0.7210 - val_auc: 0.7784\n",
            "Epoch 51/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5252 - accuracy: 0.7218 - auc: 0.7899 - val_loss: 0.5316 - val_accuracy: 0.7208 - val_auc: 0.7784\n",
            "Epoch 52/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5251 - accuracy: 0.7220 - auc: 0.7900 - val_loss: 0.5316 - val_accuracy: 0.7206 - val_auc: 0.7783\n",
            "Epoch 53/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5250 - accuracy: 0.7223 - auc: 0.7900 - val_loss: 0.5316 - val_accuracy: 0.7205 - val_auc: 0.7783\n",
            "Epoch 54/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5250 - accuracy: 0.7220 - auc: 0.7900 - val_loss: 0.5315 - val_accuracy: 0.7211 - val_auc: 0.7784\n",
            "Epoch 55/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5250 - accuracy: 0.7220 - auc: 0.7901 - val_loss: 0.5315 - val_accuracy: 0.7206 - val_auc: 0.7783\n",
            "Epoch 56/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5250 - accuracy: 0.7223 - auc: 0.7901 - val_loss: 0.5315 - val_accuracy: 0.7211 - val_auc: 0.7784\n",
            "Epoch 57/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5249 - accuracy: 0.7222 - auc: 0.7901 - val_loss: 0.5315 - val_accuracy: 0.7210 - val_auc: 0.7784\n",
            "Epoch 58/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5249 - accuracy: 0.7219 - auc: 0.7901 - val_loss: 0.5315 - val_accuracy: 0.7206 - val_auc: 0.7784\n",
            "Epoch 59/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5249 - accuracy: 0.7222 - auc: 0.7902 - val_loss: 0.5315 - val_accuracy: 0.7208 - val_auc: 0.7784\n",
            "Epoch 60/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5248 - accuracy: 0.7223 - auc: 0.7902 - val_loss: 0.5315 - val_accuracy: 0.7209 - val_auc: 0.7783\n",
            "Epoch 61/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5248 - accuracy: 0.7222 - auc: 0.7902 - val_loss: 0.5315 - val_accuracy: 0.7206 - val_auc: 0.7783\n",
            "Epoch 62/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5248 - accuracy: 0.7223 - auc: 0.7902 - val_loss: 0.5315 - val_accuracy: 0.7206 - val_auc: 0.7784\n",
            "Epoch 63/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5248 - accuracy: 0.7225 - auc: 0.7903 - val_loss: 0.5315 - val_accuracy: 0.7207 - val_auc: 0.7784\n",
            "Epoch 64/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7226 - auc: 0.7903 - val_loss: 0.5314 - val_accuracy: 0.7205 - val_auc: 0.7784\n",
            "Epoch 65/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7225 - auc: 0.7903 - val_loss: 0.5314 - val_accuracy: 0.7204 - val_auc: 0.7784\n",
            "Epoch 66/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7226 - auc: 0.7903 - val_loss: 0.5314 - val_accuracy: 0.7210 - val_auc: 0.7784\n",
            "Epoch 67/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7227 - auc: 0.7903 - val_loss: 0.5314 - val_accuracy: 0.7209 - val_auc: 0.7784\n",
            "Epoch 68/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7227 - auc: 0.7903 - val_loss: 0.5314 - val_accuracy: 0.7207 - val_auc: 0.7784\n",
            "Epoch 69/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7225 - auc: 0.7903 - val_loss: 0.5314 - val_accuracy: 0.7206 - val_auc: 0.7784\n",
            "Epoch 70/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5247 - accuracy: 0.7225 - auc: 0.7903 - val_loss: 0.5314 - val_accuracy: 0.7206 - val_auc: 0.7784\n",
            "Epoch 71/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5246 - accuracy: 0.7226 - auc: 0.7903 - val_loss: 0.5314 - val_accuracy: 0.7204 - val_auc: 0.7784\n",
            "Epoch 72/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5246 - accuracy: 0.7224 - auc: 0.7903 - val_loss: 0.5314 - val_accuracy: 0.7202 - val_auc: 0.7784\n",
            "Epoch 73/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5246 - accuracy: 0.7224 - auc: 0.7904 - val_loss: 0.5314 - val_accuracy: 0.7201 - val_auc: 0.7783\n",
            "Epoch 74/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5246 - accuracy: 0.7225 - auc: 0.7904 - val_loss: 0.5314 - val_accuracy: 0.7202 - val_auc: 0.7783\n",
            "Epoch 75/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5246 - accuracy: 0.7226 - auc: 0.7904 - val_loss: 0.5314 - val_accuracy: 0.7205 - val_auc: 0.7784\n",
            "Epoch 76/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5246 - accuracy: 0.7227 - auc: 0.7904 - val_loss: 0.5314 - val_accuracy: 0.7203 - val_auc: 0.7784\n",
            "Epoch 77/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5246 - accuracy: 0.7226 - auc: 0.7904 - val_loss: 0.5314 - val_accuracy: 0.7202 - val_auc: 0.7784\n",
            "Epoch 78/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5246 - accuracy: 0.7226 - auc: 0.7904 - val_loss: 0.5314 - val_accuracy: 0.7203 - val_auc: 0.7784\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00078: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f20ee1be090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvrewA3s_5da",
        "outputId": "e021eb11-7526-406c-9fdf-c08fa5c5bd0d"
      },
      "source": [
        "results = []\n",
        "results.append(('Initial DNN',roc_auc_score(y_test,model.predict(X_test))))\n",
        "print(results[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Initial DNN', 0.7803878280326728)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XcDGLRQgUxF"
      },
      "source": [
        "# Aletheia Unwrapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtIIPuzjd7Yp"
      },
      "source": [
        "from aletheia import *\n",
        "\n",
        "coefs = [layer.kernel.numpy() for layer in model.layers]\n",
        "intercepts = [layer.bias.numpy() for layer in model.layers]\n",
        "clf = UnwrapperClassifier(coefs, intercepts,feature_names=fnames)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iXSkJDjd7Yp"
      },
      "source": [
        "# print(clf.summary().shape)  #dont run.crashes pc. Object too big. Need to optimize dnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "mQNoycVrd7Yq",
        "outputId": "2d5488c5-64a9-4de4-cbca-b0f0a5200980"
      },
      "source": [
        "#feature importance plot \n",
        "fig = clf.feature_importance_plot(figsize=(100, 20),include_b0=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 7200x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZfxgySkgixs"
      },
      "source": [
        "# Merging LLMs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js693sJzd7Yq",
        "outputId": "66141ec2-4059-4cc1-def7-fb15f60f748c"
      },
      "source": [
        "#Using grid search to find optimal number of LLM clusters\n",
        "\n",
        "datanum = X_train.shape[0]\n",
        "indices = np.arange(datanum)\n",
        "idx1, idx2 = train_test_split(indices, test_size=0.2, random_state=0)\n",
        "val_fold = np.ones((len(indices)))\n",
        "val_fold[idx1] = -1\n",
        "\n",
        "X_train_np = X_train.values\n",
        "y_train_np = y_train.values\n",
        "\n",
        "grid = GridSearchCV(MergerClassifier(unwrapper=None, \n",
        "                                     weights=coefs,\n",
        "                                     biases=intercepts, \n",
        "                                     min_samples=50,\n",
        "                                     n_neighbors=np.round(clf.nllms * 0.01).astype(int),\n",
        "                                     refit_model=LogisticRegression()),\n",
        "                                     param_grid={\"n_clusters\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]},\n",
        "                                     scoring={\"auc\": make_scorer(roc_auc_score, needs_proba=True)},\n",
        "                                     cv=PredefinedSplit(val_fold), refit=\"auc\", error_score=np.nan, verbose=3)\n",
        "# grid = MergerClassifier(unwrapper=None,\n",
        "#                       weights=coefs,\n",
        "#                       biases=intercepts,\n",
        "#                       min_samples=30,\n",
        "#                       n_neighbors=np.round(clf1.nllms * 0.01).astype(int),\n",
        "#                       refit_model=LogisticRegression(),\n",
        "#                       n_clusters=10\n",
        "#                      )\n",
        "grid.fit(X_train_np, y_train_np)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 12 candidates, totalling 12 fits\n",
            "[CV 1/1] END .................n_clusters=1; auc: (test=0.748) total time= 3.1min\n",
            "[CV 1/1] END .................n_clusters=2; auc: (test=0.768) total time= 3.2min\n",
            "[CV 1/1] END .................n_clusters=3; auc: (test=0.774) total time= 3.2min\n",
            "[CV 1/1] END .................n_clusters=4; auc: (test=0.774) total time= 3.4min\n",
            "[CV 1/1] END .................n_clusters=5; auc: (test=0.776) total time= 3.5min\n",
            "[CV 1/1] END .................n_clusters=6; auc: (test=0.775) total time= 3.4min\n",
            "[CV 1/1] END .................n_clusters=7; auc: (test=0.774) total time= 3.5min\n",
            "[CV 1/1] END .................n_clusters=8; auc: (test=0.774) total time= 3.5min\n",
            "[CV 1/1] END .................n_clusters=9; auc: (test=0.774) total time= 3.3min\n",
            "[CV 1/1] END ................n_clusters=10; auc: (test=0.775) total time= 3.4min\n",
            "[CV 1/1] END ................n_clusters=15; auc: (test=0.771) total time= 3.5min\n",
            "[CV 1/1] END ................n_clusters=20; auc: (test=0.769) total time= 4.7min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1,  1])),\n",
              "             estimator=MergerClassifier(biases=[array([ 0.05564463, -0.07092811,  0.23131742,  0.0918405 , -0.10335322,\n",
              "        0.05048175, -0.00245235, -0.04160368, -0.07714047,  0.08638024,\n",
              "        0.10046817,  0.00740497,  0.07705621,  0.05546864,  0.00968216,\n",
              "       -0.03915994,  0.1975416 ,  0.08697756,  0.20055805,  0.24345444,\n",
              "        0.03496301, -0.036...\n",
              "       [-0.23368052],\n",
              "       [-0.16968824],\n",
              "       [-0.3584583 ],\n",
              "       [-0.45079887],\n",
              "       [ 0.21265517],\n",
              "       [ 0.20515855],\n",
              "       [ 0.8851064 ],\n",
              "       [-0.61266315],\n",
              "       [-0.10798008],\n",
              "       [ 0.38765153],\n",
              "       [ 0.7343377 ],\n",
              "       [ 0.65345675],\n",
              "       [-0.2068829 ],\n",
              "       [-0.49876758],\n",
              "       [ 0.41272715]], dtype=float32)]),\n",
              "             param_grid={'n_clusters': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]},\n",
              "             refit='auc',\n",
              "             scoring={'auc': make_scorer(roc_auc_score, needs_proba=True)},\n",
              "             verbose=3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLgldgAYd7Yr"
      },
      "source": [
        "#save grid file\n",
        "with open('grid.pkl', 'wb') as output:\n",
        "    joblib.dump(grid, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRzQC3_4d7Yr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "ad71c608-2498-4187-d025-d0846cc5b2c0"
      },
      "source": [
        "# results on different n_cluster values\n",
        "pd.DataFrame(grid.cv_results_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_n_clusters</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_auc</th>\n",
              "      <th>mean_test_auc</th>\n",
              "      <th>std_test_auc</th>\n",
              "      <th>rank_test_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>171.856152</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.070130</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_clusters': 1}</td>\n",
              "      <td>0.748234</td>\n",
              "      <td>0.748234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>178.210827</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.090551</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_clusters': 2}</td>\n",
              "      <td>0.767673</td>\n",
              "      <td>0.767673</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>178.661271</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.956981</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>{'n_clusters': 3}</td>\n",
              "      <td>0.773530</td>\n",
              "      <td>0.773530</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>186.636152</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.928843</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>{'n_clusters': 4}</td>\n",
              "      <td>0.773668</td>\n",
              "      <td>0.773668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>194.573988</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.280597</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>{'n_clusters': 5}</td>\n",
              "      <td>0.775545</td>\n",
              "      <td>0.775545</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>189.914986</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.912681</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>{'n_clusters': 6}</td>\n",
              "      <td>0.775454</td>\n",
              "      <td>0.775454</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>196.290391</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.326067</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>{'n_clusters': 7}</td>\n",
              "      <td>0.774476</td>\n",
              "      <td>0.774476</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>193.254446</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.056786</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "      <td>{'n_clusters': 8}</td>\n",
              "      <td>0.774347</td>\n",
              "      <td>0.774347</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>185.735209</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.868467</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9</td>\n",
              "      <td>{'n_clusters': 9}</td>\n",
              "      <td>0.774346</td>\n",
              "      <td>0.774346</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>190.024789</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.522214</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10</td>\n",
              "      <td>{'n_clusters': 10}</td>\n",
              "      <td>0.774755</td>\n",
              "      <td>0.774755</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>191.310814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.754946</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15</td>\n",
              "      <td>{'n_clusters': 15}</td>\n",
              "      <td>0.771361</td>\n",
              "      <td>0.771361</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>263.881402</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.270261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20</td>\n",
              "      <td>{'n_clusters': 20}</td>\n",
              "      <td>0.769427</td>\n",
              "      <td>0.769427</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mean_fit_time  std_fit_time  ...  std_test_auc  rank_test_auc\n",
              "0      171.856152           0.0  ...           0.0             12\n",
              "1      178.210827           0.0  ...           0.0             11\n",
              "2      178.661271           0.0  ...           0.0              8\n",
              "3      186.636152           0.0  ...           0.0              7\n",
              "4      194.573988           0.0  ...           0.0              1\n",
              "5      189.914986           0.0  ...           0.0              2\n",
              "6      196.290391           0.0  ...           0.0              4\n",
              "7      193.254446           0.0  ...           0.0              5\n",
              "8      185.735209           0.0  ...           0.0              6\n",
              "9      190.024789           0.0  ...           0.0              3\n",
              "10     191.310814           0.0  ...           0.0              9\n",
              "11     263.881402           0.0  ...           0.0             10\n",
              "\n",
              "[12 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E1e0ehvd7Yr"
      },
      "source": [
        "#choose best n_cluster value\n",
        "clf_merge = grid.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC-qYZ5gd7Yr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "9f6524d9-c75a-4928-9bdb-3c723a4d369b"
      },
      "source": [
        "#Merged DNN unwrapped\n",
        "clf_merge.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Count</th>\n",
              "      <th>Response Mean</th>\n",
              "      <th>Response Std</th>\n",
              "      <th>Local AUC</th>\n",
              "      <th>Global AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14693.0</td>\n",
              "      <td>0.324917</td>\n",
              "      <td>0.468344</td>\n",
              "      <td>0.614702</td>\n",
              "      <td>0.563184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5647.0</td>\n",
              "      <td>0.892686</td>\n",
              "      <td>0.309512</td>\n",
              "      <td>0.849899</td>\n",
              "      <td>0.701832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5628.0</td>\n",
              "      <td>0.417022</td>\n",
              "      <td>0.493067</td>\n",
              "      <td>0.613877</td>\n",
              "      <td>0.731151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2155.0</td>\n",
              "      <td>0.822274</td>\n",
              "      <td>0.382282</td>\n",
              "      <td>0.796636</td>\n",
              "      <td>0.698664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1667.0</td>\n",
              "      <td>0.571086</td>\n",
              "      <td>0.494921</td>\n",
              "      <td>0.665095</td>\n",
              "      <td>0.730853</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Count  Response Mean  Response Std  Local AUC  Global AUC\n",
              "0  14693.0       0.324917      0.468344   0.614702    0.563184\n",
              "1   5647.0       0.892686      0.309512   0.849899    0.701832\n",
              "2   5628.0       0.417022      0.493067   0.613877    0.731151\n",
              "3   2155.0       0.822274      0.382282   0.796636    0.698664\n",
              "4   1667.0       0.571086      0.494921   0.665095    0.730853"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db0GzXj4Pr1m"
      },
      "source": [
        "## Getting Flattened Model with one hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "legLuP-OCDQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18cb0121-638d-4ecf-e008-22e84d9ab08e"
      },
      "source": [
        "flatten_model = clf_merge.flatten(refit_model=LogisticRegression(C=1e10))\n",
        "clf_flatten = flatten_model\n",
        "clf_flatten.fit(x_res.values, y_res.values.flatten())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azkH1u9bPBng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0d85ae-4d2c-4765-cf01-af8130e59237"
      },
      "source": [
        "results.append(('Flattened DNN', roc_auc_score(y_test,clf_flatten.predict_proba(X_test)[:,1])))\n",
        "results[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Flattened DNN', 0.7860829768619458)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu7kV_PjPV0w"
      },
      "source": [
        "## Training new DNN same architecture as Flattened DNN (for comparison)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ypjuTeed7Ys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc280968-13cf-41ac-993c-83526ab68537"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "metrics = [ \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n",
        "ES=tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\",patience=20,restore_best_weights=True, mode='max', verbose=1)\n",
        "\n",
        "Rlr =tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,\n",
        "                              patience=10, min_lr=0.000001)\n",
        "\n",
        "def create_model(input_shape = (X_train.shape[1],)):\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(5,activation='relu',input_shape=input_shape))\n",
        "    model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',loss = 'binary_crossentropy', metrics = metrics)\n",
        "    return model\n",
        "              \n",
        "# model = KerasClassifier(create_model, verbose=1)\n",
        "model = create_model()\n",
        "\n",
        "\n",
        "model.fit(x=X_train,\n",
        "          y=y_train,\n",
        "          epochs = 500,\n",
        "          batch_size=512,\n",
        "          validation_data =(X_val, y_val),\n",
        "         callbacks=[ES, Rlr],\n",
        "#           class_weight = {0: zero_weight, 1: one_weight}\n",
        "         )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "59/59 [==============================] - 2s 13ms/step - loss: 0.7830 - accuracy: 0.5258 - auc: 0.5409 - prc: 0.5397 - val_loss: 0.7333 - val_accuracy: 0.5493 - val_auc: 0.5709 - val_prc: 0.5636\n",
            "Epoch 2/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.7058 - accuracy: 0.5752 - auc: 0.6051 - prc: 0.6015 - val_loss: 0.6777 - val_accuracy: 0.6105 - val_auc: 0.6447 - val_prc: 0.6398\n",
            "Epoch 3/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.6656 - accuracy: 0.6205 - auc: 0.6609 - prc: 0.6648 - val_loss: 0.6461 - val_accuracy: 0.6465 - val_auc: 0.6924 - val_prc: 0.7005\n",
            "Epoch 4/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.6414 - accuracy: 0.6483 - auc: 0.6940 - prc: 0.7080 - val_loss: 0.6258 - val_accuracy: 0.6688 - val_auc: 0.7195 - val_prc: 0.7383\n",
            "Epoch 5/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.6257 - accuracy: 0.6627 - auc: 0.7127 - prc: 0.7347 - val_loss: 0.6122 - val_accuracy: 0.6772 - val_auc: 0.7333 - val_prc: 0.7594\n",
            "Epoch 6/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.6148 - accuracy: 0.6709 - auc: 0.7240 - prc: 0.7519 - val_loss: 0.6028 - val_accuracy: 0.6828 - val_auc: 0.7408 - val_prc: 0.7711\n",
            "Epoch 7/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.6068 - accuracy: 0.6763 - auc: 0.7309 - prc: 0.7622 - val_loss: 0.5955 - val_accuracy: 0.6880 - val_auc: 0.7467 - val_prc: 0.7806\n",
            "Epoch 8/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.6005 - accuracy: 0.6822 - auc: 0.7366 - prc: 0.7711 - val_loss: 0.5896 - val_accuracy: 0.6909 - val_auc: 0.7505 - val_prc: 0.7867\n",
            "Epoch 9/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5950 - accuracy: 0.6849 - auc: 0.7402 - prc: 0.7765 - val_loss: 0.5842 - val_accuracy: 0.6933 - val_auc: 0.7537 - val_prc: 0.7915\n",
            "Epoch 10/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5900 - accuracy: 0.6885 - auc: 0.7436 - prc: 0.7818 - val_loss: 0.5791 - val_accuracy: 0.6966 - val_auc: 0.7563 - val_prc: 0.7957\n",
            "Epoch 11/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5855 - accuracy: 0.6903 - auc: 0.7459 - prc: 0.7856 - val_loss: 0.5748 - val_accuracy: 0.6989 - val_auc: 0.7583 - val_prc: 0.7992\n",
            "Epoch 12/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5822 - accuracy: 0.6938 - auc: 0.7478 - prc: 0.7881 - val_loss: 0.5728 - val_accuracy: 0.7004 - val_auc: 0.7592 - val_prc: 0.8005\n",
            "Epoch 13/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5803 - accuracy: 0.6943 - auc: 0.7489 - prc: 0.7896 - val_loss: 0.5709 - val_accuracy: 0.7003 - val_auc: 0.7598 - val_prc: 0.8017\n",
            "Epoch 14/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5785 - accuracy: 0.6955 - auc: 0.7498 - prc: 0.7909 - val_loss: 0.5691 - val_accuracy: 0.7011 - val_auc: 0.7606 - val_prc: 0.8028\n",
            "Epoch 15/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5767 - accuracy: 0.6962 - auc: 0.7510 - prc: 0.7922 - val_loss: 0.5674 - val_accuracy: 0.7028 - val_auc: 0.7614 - val_prc: 0.8039\n",
            "Epoch 16/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5750 - accuracy: 0.6961 - auc: 0.7518 - prc: 0.7934 - val_loss: 0.5658 - val_accuracy: 0.7030 - val_auc: 0.7620 - val_prc: 0.8048\n",
            "Epoch 17/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5735 - accuracy: 0.6962 - auc: 0.7526 - prc: 0.7943 - val_loss: 0.5644 - val_accuracy: 0.7050 - val_auc: 0.7627 - val_prc: 0.8056\n",
            "Epoch 18/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5720 - accuracy: 0.6967 - auc: 0.7535 - prc: 0.7954 - val_loss: 0.5630 - val_accuracy: 0.7057 - val_auc: 0.7634 - val_prc: 0.8064\n",
            "Epoch 19/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5706 - accuracy: 0.6978 - auc: 0.7543 - prc: 0.7963 - val_loss: 0.5617 - val_accuracy: 0.7060 - val_auc: 0.7640 - val_prc: 0.8072\n",
            "Epoch 20/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5693 - accuracy: 0.6983 - auc: 0.7550 - prc: 0.7972 - val_loss: 0.5604 - val_accuracy: 0.7079 - val_auc: 0.7646 - val_prc: 0.8078\n",
            "Epoch 21/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5682 - accuracy: 0.6997 - auc: 0.7557 - prc: 0.7979 - val_loss: 0.5593 - val_accuracy: 0.7074 - val_auc: 0.7651 - val_prc: 0.8083\n",
            "Epoch 22/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5672 - accuracy: 0.7009 - auc: 0.7563 - prc: 0.7985 - val_loss: 0.5589 - val_accuracy: 0.7075 - val_auc: 0.7653 - val_prc: 0.8086\n",
            "Epoch 23/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5667 - accuracy: 0.7013 - auc: 0.7566 - prc: 0.7988 - val_loss: 0.5584 - val_accuracy: 0.7080 - val_auc: 0.7655 - val_prc: 0.8089\n",
            "Epoch 24/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5662 - accuracy: 0.7019 - auc: 0.7569 - prc: 0.7992 - val_loss: 0.5579 - val_accuracy: 0.7078 - val_auc: 0.7658 - val_prc: 0.8090\n",
            "Epoch 25/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5657 - accuracy: 0.7019 - auc: 0.7572 - prc: 0.7993 - val_loss: 0.5574 - val_accuracy: 0.7081 - val_auc: 0.7659 - val_prc: 0.8093\n",
            "Epoch 26/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5652 - accuracy: 0.7023 - auc: 0.7576 - prc: 0.7997 - val_loss: 0.5570 - val_accuracy: 0.7088 - val_auc: 0.7662 - val_prc: 0.8095\n",
            "Epoch 27/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5648 - accuracy: 0.7025 - auc: 0.7578 - prc: 0.8000 - val_loss: 0.5566 - val_accuracy: 0.7091 - val_auc: 0.7665 - val_prc: 0.8098\n",
            "Epoch 28/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5643 - accuracy: 0.7027 - auc: 0.7581 - prc: 0.8002 - val_loss: 0.5561 - val_accuracy: 0.7100 - val_auc: 0.7667 - val_prc: 0.8100\n",
            "Epoch 29/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5639 - accuracy: 0.7023 - auc: 0.7583 - prc: 0.8004 - val_loss: 0.5556 - val_accuracy: 0.7101 - val_auc: 0.7670 - val_prc: 0.8103\n",
            "Epoch 30/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5635 - accuracy: 0.7027 - auc: 0.7586 - prc: 0.8007 - val_loss: 0.5552 - val_accuracy: 0.7111 - val_auc: 0.7671 - val_prc: 0.8104\n",
            "Epoch 31/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5631 - accuracy: 0.7027 - auc: 0.7589 - prc: 0.8009 - val_loss: 0.5548 - val_accuracy: 0.7108 - val_auc: 0.7673 - val_prc: 0.8106\n",
            "Epoch 32/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5628 - accuracy: 0.7029 - auc: 0.7591 - prc: 0.8011 - val_loss: 0.5546 - val_accuracy: 0.7113 - val_auc: 0.7673 - val_prc: 0.8107\n",
            "Epoch 33/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5626 - accuracy: 0.7032 - auc: 0.7592 - prc: 0.8013 - val_loss: 0.5544 - val_accuracy: 0.7114 - val_auc: 0.7674 - val_prc: 0.8107\n",
            "Epoch 34/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5624 - accuracy: 0.7034 - auc: 0.7593 - prc: 0.8013 - val_loss: 0.5542 - val_accuracy: 0.7109 - val_auc: 0.7676 - val_prc: 0.8109\n",
            "Epoch 35/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5622 - accuracy: 0.7033 - auc: 0.7594 - prc: 0.8014 - val_loss: 0.5540 - val_accuracy: 0.7110 - val_auc: 0.7677 - val_prc: 0.8110\n",
            "Epoch 36/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5620 - accuracy: 0.7035 - auc: 0.7595 - prc: 0.8015 - val_loss: 0.5538 - val_accuracy: 0.7111 - val_auc: 0.7679 - val_prc: 0.8112\n",
            "Epoch 37/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5619 - accuracy: 0.7037 - auc: 0.7596 - prc: 0.8016 - val_loss: 0.5536 - val_accuracy: 0.7112 - val_auc: 0.7680 - val_prc: 0.8112\n",
            "Epoch 38/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5617 - accuracy: 0.7038 - auc: 0.7597 - prc: 0.8016 - val_loss: 0.5535 - val_accuracy: 0.7110 - val_auc: 0.7680 - val_prc: 0.8112\n",
            "Epoch 39/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5615 - accuracy: 0.7038 - auc: 0.7599 - prc: 0.8017 - val_loss: 0.5533 - val_accuracy: 0.7109 - val_auc: 0.7682 - val_prc: 0.8113\n",
            "Epoch 40/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5613 - accuracy: 0.7038 - auc: 0.7600 - prc: 0.8018 - val_loss: 0.5531 - val_accuracy: 0.7106 - val_auc: 0.7683 - val_prc: 0.8114\n",
            "Epoch 41/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5612 - accuracy: 0.7040 - auc: 0.7601 - prc: 0.8019 - val_loss: 0.5529 - val_accuracy: 0.7109 - val_auc: 0.7684 - val_prc: 0.8116\n",
            "Epoch 42/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5610 - accuracy: 0.7042 - auc: 0.7603 - prc: 0.8021 - val_loss: 0.5528 - val_accuracy: 0.7108 - val_auc: 0.7684 - val_prc: 0.8116\n",
            "Epoch 43/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5609 - accuracy: 0.7043 - auc: 0.7603 - prc: 0.8021 - val_loss: 0.5527 - val_accuracy: 0.7110 - val_auc: 0.7685 - val_prc: 0.8116\n",
            "Epoch 44/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5609 - accuracy: 0.7041 - auc: 0.7604 - prc: 0.8021 - val_loss: 0.5526 - val_accuracy: 0.7113 - val_auc: 0.7686 - val_prc: 0.8116\n",
            "Epoch 45/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5608 - accuracy: 0.7041 - auc: 0.7604 - prc: 0.8022 - val_loss: 0.5525 - val_accuracy: 0.7114 - val_auc: 0.7685 - val_prc: 0.8117\n",
            "Epoch 46/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5607 - accuracy: 0.7041 - auc: 0.7605 - prc: 0.8022 - val_loss: 0.5525 - val_accuracy: 0.7116 - val_auc: 0.7686 - val_prc: 0.8117\n",
            "Epoch 47/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5606 - accuracy: 0.7040 - auc: 0.7605 - prc: 0.8023 - val_loss: 0.5524 - val_accuracy: 0.7114 - val_auc: 0.7686 - val_prc: 0.8116\n",
            "Epoch 48/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5605 - accuracy: 0.7037 - auc: 0.7606 - prc: 0.8023 - val_loss: 0.5523 - val_accuracy: 0.7119 - val_auc: 0.7686 - val_prc: 0.8117\n",
            "Epoch 49/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5605 - accuracy: 0.7038 - auc: 0.7606 - prc: 0.8024 - val_loss: 0.5522 - val_accuracy: 0.7120 - val_auc: 0.7687 - val_prc: 0.8118\n",
            "Epoch 50/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5604 - accuracy: 0.7039 - auc: 0.7607 - prc: 0.8025 - val_loss: 0.5521 - val_accuracy: 0.7119 - val_auc: 0.7688 - val_prc: 0.8118\n",
            "Epoch 51/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5603 - accuracy: 0.7039 - auc: 0.7607 - prc: 0.8025 - val_loss: 0.5520 - val_accuracy: 0.7120 - val_auc: 0.7688 - val_prc: 0.8119\n",
            "Epoch 52/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5602 - accuracy: 0.7037 - auc: 0.7607 - prc: 0.8025 - val_loss: 0.5520 - val_accuracy: 0.7118 - val_auc: 0.7689 - val_prc: 0.8119\n",
            "Epoch 53/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5602 - accuracy: 0.7037 - auc: 0.7608 - prc: 0.8025 - val_loss: 0.5519 - val_accuracy: 0.7118 - val_auc: 0.7689 - val_prc: 0.8119\n",
            "Epoch 54/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5602 - accuracy: 0.7039 - auc: 0.7608 - prc: 0.8026 - val_loss: 0.5519 - val_accuracy: 0.7117 - val_auc: 0.7690 - val_prc: 0.8119\n",
            "Epoch 55/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5601 - accuracy: 0.7038 - auc: 0.7608 - prc: 0.8026 - val_loss: 0.5518 - val_accuracy: 0.7118 - val_auc: 0.7690 - val_prc: 0.8119\n",
            "Epoch 56/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5601 - accuracy: 0.7040 - auc: 0.7609 - prc: 0.8026 - val_loss: 0.5518 - val_accuracy: 0.7121 - val_auc: 0.7691 - val_prc: 0.8120\n",
            "Epoch 57/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5601 - accuracy: 0.7039 - auc: 0.7609 - prc: 0.8026 - val_loss: 0.5518 - val_accuracy: 0.7120 - val_auc: 0.7691 - val_prc: 0.8120\n",
            "Epoch 58/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5600 - accuracy: 0.7039 - auc: 0.7609 - prc: 0.8027 - val_loss: 0.5517 - val_accuracy: 0.7120 - val_auc: 0.7691 - val_prc: 0.8120\n",
            "Epoch 59/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5600 - accuracy: 0.7039 - auc: 0.7609 - prc: 0.8027 - val_loss: 0.5517 - val_accuracy: 0.7119 - val_auc: 0.7691 - val_prc: 0.8120\n",
            "Epoch 60/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5600 - accuracy: 0.7039 - auc: 0.7610 - prc: 0.8027 - val_loss: 0.5516 - val_accuracy: 0.7118 - val_auc: 0.7692 - val_prc: 0.8121\n",
            "Epoch 61/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5599 - accuracy: 0.7041 - auc: 0.7610 - prc: 0.8027 - val_loss: 0.5516 - val_accuracy: 0.7118 - val_auc: 0.7692 - val_prc: 0.8121\n",
            "Epoch 62/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5599 - accuracy: 0.7039 - auc: 0.7610 - prc: 0.8028 - val_loss: 0.5516 - val_accuracy: 0.7118 - val_auc: 0.7692 - val_prc: 0.8121\n",
            "Epoch 63/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5599 - accuracy: 0.7041 - auc: 0.7610 - prc: 0.8028 - val_loss: 0.5515 - val_accuracy: 0.7119 - val_auc: 0.7692 - val_prc: 0.8121\n",
            "Epoch 64/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.5598 - accuracy: 0.7039 - auc: 0.7610 - prc: 0.8028 - val_loss: 0.5515 - val_accuracy: 0.7119 - val_auc: 0.7692 - val_prc: 0.8121\n",
            "Epoch 65/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.7039 - auc: 0.7610 - prc: 0.8028 - val_loss: 0.5515 - val_accuracy: 0.7118 - val_auc: 0.7692 - val_prc: 0.8121\n",
            "Epoch 66/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.7040 - auc: 0.7611 - prc: 0.8028 - val_loss: 0.5515 - val_accuracy: 0.7118 - val_auc: 0.7692 - val_prc: 0.8121\n",
            "Epoch 67/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.7040 - auc: 0.7611 - prc: 0.8028 - val_loss: 0.5514 - val_accuracy: 0.7120 - val_auc: 0.7692 - val_prc: 0.8121\n",
            "Epoch 68/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.7039 - auc: 0.7611 - prc: 0.8028 - val_loss: 0.5514 - val_accuracy: 0.7121 - val_auc: 0.7693 - val_prc: 0.8122\n",
            "Epoch 69/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5598 - accuracy: 0.7040 - auc: 0.7611 - prc: 0.8029 - val_loss: 0.5514 - val_accuracy: 0.7121 - val_auc: 0.7693 - val_prc: 0.8122\n",
            "Epoch 70/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.7040 - auc: 0.7611 - prc: 0.8029 - val_loss: 0.5514 - val_accuracy: 0.7121 - val_auc: 0.7693 - val_prc: 0.8122\n",
            "Epoch 71/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.7040 - auc: 0.7611 - prc: 0.8029 - val_loss: 0.5514 - val_accuracy: 0.7119 - val_auc: 0.7693 - val_prc: 0.8122\n",
            "Epoch 72/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.7040 - auc: 0.7611 - prc: 0.8029 - val_loss: 0.5514 - val_accuracy: 0.7120 - val_auc: 0.7693 - val_prc: 0.8122\n",
            "Epoch 73/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.7040 - auc: 0.7611 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7693 - val_prc: 0.8122\n",
            "Epoch 74/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.7040 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7693 - val_prc: 0.8122\n",
            "Epoch 75/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.7040 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7693 - val_prc: 0.8122\n",
            "Epoch 76/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.7040 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7693 - val_prc: 0.8122\n",
            "Epoch 77/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8122\n",
            "Epoch 78/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8122\n",
            "Epoch 79/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7040 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8122\n",
            "Epoch 80/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7040 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8122\n",
            "Epoch 81/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 82/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5513 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 83/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 84/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 85/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 86/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 87/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 88/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 89/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 90/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7119 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 91/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7119 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 92/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7119 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 93/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7119 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 94/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7119 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 95/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7119 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 96/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7119 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 97/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5596 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7119 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 98/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7120 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 99/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 100/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5596 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 101/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 102/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 103/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 104/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8030 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 105/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 106/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 107/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 108/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 109/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 110/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 111/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 112/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 113/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 114/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 115/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 116/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 117/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 118/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5512 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 119/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 120/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 121/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 122/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 123/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 124/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 125/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 126/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 127/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 128/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 129/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 130/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7039 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 131/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 132/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 133/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 134/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 135/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 136/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 137/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 138/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 139/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 140/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 141/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 142/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7037 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 143/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 144/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 145/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 146/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 147/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7037 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7694 - val_prc: 0.8123\n",
            "Epoch 148/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 149/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7037 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 150/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7037 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 151/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7037 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 152/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7037 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 153/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7037 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 154/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7037 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 155/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7037 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 156/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 157/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 158/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 159/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 160/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 161/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 162/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 163/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 164/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 165/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 166/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 167/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 168/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 169/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 170/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 171/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 172/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 173/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 174/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 175/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 176/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 177/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 178/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 179/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7612 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 180/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7613 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 181/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7613 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 182/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7613 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 183/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7613 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 184/500\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7613 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Epoch 185/500\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.5595 - accuracy: 0.7038 - auc: 0.7613 - prc: 0.8029 - val_loss: 0.5511 - val_accuracy: 0.7121 - val_auc: 0.7695 - val_prc: 0.8123\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00185: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f20e0add0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVfkfveWd7Ys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a3ea04-8770-4196-b5ca-be4cfc7ad4f8"
      },
      "source": [
        "results.append(('Small DNN', roc_auc_score(y_test,model.predict(X_test)))) \n",
        "print(results[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Small DNN', 0.7689493302090246)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUmWSUYQWl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "887f74f1-7187-4b54-eccc-59742a670ab4"
      },
      "source": [
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Initial DNN', 0.7803878280326728), ('Flattened DNN', 0.7860829768619458), ('Small DNN', 0.7689493302090246)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJx9EVx2d7Yt"
      },
      "source": [
        "#saving untrained flattened model\n",
        "with open(\"untrained_model.pkl\", \"wb\") as output:\n",
        "    joblib.dump(flatten_model, output)\n",
        "\n",
        "with open(\"trained_model.pkl\", \"wb\") as output:\n",
        "    joblib.dump(clf_flatten, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJUmYW0qd7Yt"
      },
      "source": [],
      "execution_count": 1,
      "outputs": []
    },
    {
      "source": [
        "# Interpretation of test cases"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_figure(fig, filename):\n",
        "    filename = './plots/' + filename\n",
        "    fig.savefig(filename, transparent=False)\n",
        "    mpld3.save_html(fig, filename+\".html\", template_type='simple')\n",
        "    mpld3.save_json(fig, filename+\".json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import lime\n",
        "# from lime import lime_tabular\n",
        "\n",
        "explainer = lime_tabular.LimeTabularExplainer(\n",
        "    training_data=np.array(X_train),\n",
        "    feature_names=fnames,\n",
        "    class_names=['will default', 'will repay'],\n",
        "    mode='classification',\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dill as pick\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save explainer\n",
        "with open('explainer.pkl', 'wb') as output:\n",
        "    pick.dump(explainer, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#read explainer\n",
        "with open('explainer.pkl', 'rb') as input:\n",
        "  expl = pick.load(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = X_test.loc[X_test['AMT_CREDIT'] == 0]\n",
        "len(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exp = expl.explain_instance(\n",
        "    data_row=X_test.iloc[2], \n",
        "    predict_fn=clf_flatten.predict_proba,\n",
        "    # num_features=5\n",
        ")\n",
        "\n",
        "exp.show_in_notebook(show_table=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def individual_interpretation(X_test, clf_flatten, applicationID):\n",
        "  data_row = X_test.loc[X_test['Application_ID'] == applicationID]\n",
        "  if len(data_row) == 0:\n",
        "    return -1\n",
        "  exp = explainer.explain_instance(\n",
        "      data_row=data_row,\n",
        "      predict_fn=clf_flatten.predict_proba\n",
        "  )\n",
        "  fig = exp.as_pyplot_figure()\n",
        "  save_figure(fig, 'Interpretation' + str(applicatoinID))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# exp.as_html()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# exp.as_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# exp.as_map()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = exp.as_pyplot_figure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_figure(fig, 'Interpretation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#this notebook saves 5 pickle files\n",
        "# 2 files for pretrained test data\n",
        "# 1 file for grid\n",
        "# 1 file for trained model\n",
        "# 1 file for untrained model"
      ]
    }
  ]
}